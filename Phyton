import torch
from transformers import AutoTokenizer, AutoModelForCausalLM

# Load pre-trained GPT-2 model and tokenizer
model_name = "gpt2"
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForCausalLM.from_pretrained(model_name)

# Load and preprocess customer service data
data_path = "customer_service_data.txt"
with open(data_path, "r", encoding="utf-8") as f:
    data = f.read().splitlines()

def preprocess(data):
    inputs = []
    for d in data:
        inputs.append(d.lower() + tokenizer.eos_token)
    return inputs

inputs = preprocess(data)

# Fine-tune the model on customer service data
train_dataset = torch.utils.data.TensorDataset(
    torch.tensor(tokenizer.batch_encode_plus(inputs)["input_ids"])
)

training_args = TrainingArguments(
    output_dir="./results",
    num_train_epochs=1,
    per_device_train_batch_size=1,
    save_steps=1000,
    learning_rate=5e-5,
    warmup_steps=1000,
    logging_dir="./logs",
)

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset,
)

trainer.train()

# Generate responses to customer inquiries
def generate_response(model, tokenizer, input_text):
    input_ids = tokenizer.encode(input_text, return_tensors="pt")
    output = model.generate(
        input_ids, 
        max_length=100, 
        do_sample=True, 
        top_k=50,
        top_p=0.95, 
        temperature=0.7,
    )
    return tokenizer.decode(output[0], skip_special_tokens=True)

# Test the chatbot with a sample input
input_text = "I have a problem with my order"
response = generate_response(model, tokenizer, input_text)
print(response)
